[comment]: # (start_post)

## What is the basis of this post?

This post will contain some mathematical broccoli. Which is what, you ask? When I worked in the corporate world, I coined a term called "corporate broccoli". It's something that needs to be done and will be good for you, but for most people isn't particularly pleasant. I always thanked people for eating the corporate broccoli.

Even though I dislike real-world broccoli (corporate or vegetable), I love mathematical broccoli. The happiest times of my life were spent working through how the functions of physics fit together. Does that make me a weirdo? Read on, and form your own opinions. If you don't find it super interesting, remember that we are going to need this stuff as we get into the cooler parts of quantum mechanics and quantum computing.

So broccoli. I just showed you hopefully how we can get the value of pretty much any function using only arithmetic provided we know the function's slope, slope of the slope (curvature), slope of the slope of the slope (no familiar name...) and so on, at a single point.

Forget about the slopes for a minute - what we really did in a more general sense is represent the function with a set of "basis" functions. In the example above, the set of functions is $(x-x_0)^n$, where n goes from 0 to infinity (so an infinite number of functions.) 

Why do such a thing? Turns out, almost all calculations in physics are easier using the right basis. What a basis does is the following:

(something hard to calculate) = weighted sum(set of easy things to calculate) 

What I am doing here is a little out of order from the way you would be taught as a physics student. Usually, the first time we hear about a "basis", we start with vectors (little arrows) in our [linear algebra](https://en.wikipedia.org/wiki/Linear_algebra) class.

These arrows often represent the familiar 3-dimensions in space. Any point in space can be specified by a set of three numbers, $(x,y,z)$. And any vector position $\mathbf{r}$ is:

$$\mathbf{r}=x\hat{x} + y\hat{y} + z\hat{z}$$

My three "basis" vectors $\hat{x},\hat{y},\hat{z}$ are just the normal coordinate vectors in Cartesian space (up/down, left/right, back/front.) The little "hat" things on top of the letters say they are _orthonormal vectors_, which we will define below. Physicists love this little hat by the way, so it gets reused in other contexts, so be careful when you see it.

Rather than using sums, sometimes we represent $\mathbf{r}$ in an equivalent way with a column of numbers:

$$\mathbf{r} = \begin{bmatrix}
x \\
y \\
z 
\end{bmatrix}$$

Here $x,y,z$ are the coefficients of the basis vectors of 3-d. I'll switch back and forth between these notations if I think one is clearer than the other (or I just like one vs. the other.) They mean the same thing so it really is just a preference.

We have already seen with Taylor series that the "basis" functions can be polynomials. In fact, we can start to see some similarities if we present a basis as a set:

Cartesian basis: $\left\{\hat{x},\hat{y},\hat{z}\right\}$

Polynomial basis: $\left\{1,x,x^2,x^3,...,x^n,...\right\}$

In general, a basis set defines a "space" where you can map things. A space can be anything. For instance, we can define a "vegginess" function with vegetables as a basis:

Vegginess = a\*broccoli + b\*turnip + c\*kale

Here our basis is {broccoli,turnip,kale}, and any point in the vegginess space is three numbers, the ordered tuple (a,b,c). And the column-vector representation is as you expect:

$$\text{Vegginess} = \begin{bmatrix}
a \\
b \\
c 
\end{bmatrix}$$

##### Orthogonal basis and inner products

An inner product measures something about how much a function "looks like" another function. In the case of an orthogonal basis, the inner product of any two basis functions is zero if the functions don't exactly match. That's called _orthogonality_. These orthogonal functions look NOTHING like one another. That ensures if I change a coefficent of a basis function, I don't have to change any of the other coefficients. That's just saying, in our 3-d example,  I can move left or right independently of whether I move up/down or back/forth.

The concept of orthogonality requires a 'dot' or inner product operation (the projection of one vector onto another.) For vectors, this is written as $\mathbf{r}_1\cdot \mathbf{r}_2$.  More generally, inner products can also be written as $(r_1,r_2)$ or $\left<r_1|r_2\right>$.

Colloquially, "orthogonal" means that two vectors are pointing in completely independent directions (at "right" or 90 degree angles.) For two vectors, $\mathbf{r}_1$ and $\mathbf{r}_2$, if $\mathbf{r}_1\cdot \mathbf{r}_2=0$, they are orthogonal. 

For two vectors $r_1, r_2$ in three dimensions, the inner product can be written as:

$$\mathbf{r}_1\cdot \mathbf{r}_2 = [x_1 y_1 z_1]\begin{bmatrix}x_2 \\ y_2 \\ z_2\end{bmatrix} = x_1x_2 + y_1y_2 + z_1z_2$$

So the vector inner product matches each basis term in order and multiplies the coefficients pair-wise, then adds up to a single number. The inner product _collapses_ dimensions, taking (in our example) a pair of 3-d vectors and turning them into a simple number (a scalar.)

When we "dot" vectors, we write the first vector as a row (a "transpose") and the second as a column by convention. It will come in handy later, don't worry too much about why we do this right now.

By the way, we can see why, say, $\hat{x}$ and $\hat{y}$ are orthogonal. Their product looks like:

$$\hat{x}\cdot\hat{y} = [1 0 0]\begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix} = 1\times0 + 0\times 1 + 0\times 0 = 0$$

Let's generalize this concept so we aren't just representing a point in a 3-d space. For instance, we can map all the points in real space $x$ to a basis in another space, characterised by the parameter $k$. The basis functions $b_k(x)$ can be really just about anything, and $k$ can represent a set of integers or even continuous real numbers (or even more complicated things.) We can represent most[1](basis-footnotes) functions $f(x)$: 

$$\tilde{f}(k) = \int\limits_{S}{f(x)b_k(x)dx}$$

The function $\tilde{f}(k)$ is called the transform of $f(x)$, essentially building $f(x)$ out of our $b_k(x)$ basis functions, over some region $S$. The kooky thing here - each $k$ is like a new dimension in our above familiar breakdown of a vector in 3 dimensions. But there are an infinity of $k$'s. Pretty cool, right?

##### Normalization (the orthonormal basis) and bra-kets

Here's another thing: we want our inner products to sum to one when we take the product with a basis function. That way, we don't have to worry about keeping extra factors around. When a basis function's inner product with itself is =1 it is called "normalized." Our 3-d real space basis vectors $\hat{x},\hat{y},\hat{z}$ already have this property as we saw above.

In general, our our transform basis would have to satisfy:

$$(b_k,b_l) = \int\limits_{S}dx b_k(x)b_l(x) = \delta(k-l)$$

where $\delta(k-l)$ is a [Dirac Delta](https://en.wikipedia.org/wiki/Dirac_delta_function) function. That's the continuum equivalent of saying a basis function projects onto itself with an inner product of "one"[2](basis-footnotes), otherwise it is zero. 

The parameters $k,l$ don't have to be continuous of course - they can be discrete values in many cases. If $k,l$ are discrete, we'd get a [Kroeneker](https://en.wikipedia.org/wiki/Kronecker_delta) delta rather than a Dirac delta, but the principle is the same.

Physicists liked this idea so much, that in quantum mechanics they just said to hell with all those $f(x)$'s and what-not. All we need is a basis function that depends on quantum numbers only. If a set of quantum numbers is represented as $d$, then the basis functions describing a system can be written as:

$$\psi_d = \left| d \right>$$

So any old quantum state $\Psi$ can be described as a superposition of the $d$ basis functions:

$$\Psi = \sum\limits_{d} a_d\left|d\right>$$

where the $a_d$ are [complex numbers](https://en.wikipedia.org/wiki/Complex_number) (we'll be getting to those later.)

The $|>$ shaped things are called "kets", which is half of the pseudo-portmanteau ["bra-ket"](https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation), as in "bracket". The left $<|$-shaped things are indeed called "bras". The sophomoric jokes write themselves.

Now, the [inner-product](https://en.wikipedia.org/wiki/Inner_product_space) in quantum mechanics is a little different than just regular unit vectors in 3-d space, since they involve complex numbers. So there needs to be a "direction" to the basis functions depending on whether they form the left or right side of a product. So the left side is written:

$$\left<d\right|$$

and the inner product of a superposition state $\Psi$ with any basis function $\left|b\right>$ is:

$$\left<b|\Psi\right> = \left<b\right|\sum_d{a_d\left|d\right>} = a_b$$

which just "picks out" the coefficient of that particular basis function.

If $\left<b\right|$ has a complex number coefficient $b_b$, remember to take the [complex conjugate](https://en.wikipedia.org/wiki/Complex_conjugate). Such as:

$$\left<b|b_b^*\Psi\right> = \left<b\right|b_b^*\sum_\limits{d} a_d\left|d\right> = b_b^*a_b$$

Don't worry, we can always go back to the familiar $x$ basis if we need to (remember how we can have continuous numbers as a basis?) Something like:

$$\Psi(x) = \left<x|\Psi\right>$$

which is just the projection of $\left|x\right>$ onto $\left|\Psi\right>$. 

Now that we know about bases and inner products, let's go back to our Taylor polynomials and ask a question: what is the inner product between two Taylor polynomials? You might be tempted to try something like:

$$(x^n,x^m) = \int\limits_{-\infty}^{\infty}{x^n x^m}dx$$

but of course that doesn't work - that sum is always infinity (or zero if $n+m$ is odd). As it turns out, this set of polynomials is not orthonormal on this inner product! So you have already done some calculations in an non-orthonormal basis, which is usually considered an advanced math topic.

We are going to find out there _is_ an inner product which makes the Taylor polynomials orthonormal, but that's going to have to wait until we get to complex numbers. In the mean time, there are other ways to make polynomials orthogonal that you can read about [here](basis-footnotes).

Another question: since we defined an "inner" product, does that imply we have "outer" products to think about as well? Well, of course!

An "outer product" of, say, a pair of 2-d vectors $\mathbf{r_1},\mathbf{r_2}$ would look like:

$$\mathbf{r_1}\otimes\mathbf{r_2} = (x_1\hat{x}+y_1\hat{y})\otimes(x_2\hat{x}+y_2\hat{y})$$

Unlike the $\cdot$ inner-product operator (where we would match the basis vectors on each side of the operator) the $\otimes$ operator basically says "take all the possibilities." The result is no longer just a simple number, we have to keep everything, so it looks like:

$$\mathbf{r_1}\otimes\mathbf{r_2} = x_1x_2\hat{x}\hat{x}+x_1y_2\hat{x}\hat{y}+y_1x_2\hat{y}\hat{x}+y_1y_2\hat{y}\hat{y}$$

Those weird products of basis vectors (e.g. $\hat{x}\hat{y}$) are called _dyadics_. You probably won't ever have to work with Cartesian dyadics unless you are working on, say, computer graphics. But if we are going to really get into quantum mechanics and quantum computing, then familiarity with outer products is a must. 

Unlike inner products (which turn a pair of functions into a number, and so _collapse_ dimensions), outer products can get really big and gnarly (they _multiply_ dimensions.) For functions made of up $n_b$ basis functions, outer products can produce $n_b^d$ terms where $d$ is the number of terms in the outer product. This has a very deep implication in quantum mechanics where the set of all states don't just represent empty book-entries. The number of quantum states grows exponentially with each particle added, unlike in classical mechanics, where you only need to keep track of every new particle's position and momentum. This fact is what makes quantum computing so powerful. We'll have a lot more to say about this in future posts.

In quantum computing, we are going to see a lot of outer products. For example, for three quanta:

$$\left|\phi\right> = \left|s_1\right>\otimes\left|s_2\right>\otimes\left|s_3\right>$$

which represents the product state of three quanta in states $\{s_1,s_2,s_3\}$. For [qubits](https://en.wikipedia.org/wiki/Qubit), the states correspond to 0 and/or 1, or combinations of each. The most general case of the above outer product would thus have (two states)^(3 quanta) or $2^3=8$ terms. If we make the correspondence of states $\left|0\right>, \left|1\right>$ to 0,1 of binary bits, then the largest outer product of two-level quanta contains _all_ values of the 3 bits of the qubit register. That's very different from a classical computer, where a 3-bit register has some definite value between 0-7.  

So, in summary:

* A basis along with an inner product operation lets us create functions from standard parts that all behave in a nice way. We can reduce practically any problem in physics into finding the right basis coefficients.
* An inner product of two functions is a measure of how much one function "looks like" another. If the function is expanded in a basis set, then an inner-product of any basis function with the composite function is a measure of "how much" of that basis function it contains.
* An outer product can take two functions made up of basis functions and create all possible combinations. These combinations can get very big, very fast. As we will find out, it is how nature keeps track of gigantic sets of quanta.
* Choose basis functions wisely. And eat your broccoli.



